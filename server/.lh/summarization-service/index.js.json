{
    "sourceFile": "summarization-service/index.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 12,
            "patches": [
                {
                    "date": 1709643830178,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1709750883001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,76 @@\n+const express = require('express');\n+const { VertexAI } = require('@google-cloud/vertexai');\n+\n+const app = express();\n+\n+const {VertexAI} = require('@google-cloud/vertexai');\n+\n+// Initialize Vertex with your Cloud project and location\n+const vertex_ai = new VertexAI({project: 'inbound-domain-415706', location: 'us-east4'});\n+const model = 'gemini-1.0-pro-vision-001';\n+\n+// Instantiate the models\n+const generativeModel = vertex_ai.preview.getGenerativeModel({\n+  model: model,\n+  generation_config: {\n+    \"max_output_tokens\": 2048,\n+    \"temperature\": 0.4,\n+    \"top_p\": 1,\n+    \"top_k\": 32\n+},\n+  safety_settings: [\n+    {\n+        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n+        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+    },\n+    {\n+        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n+        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+    },\n+    {\n+        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n+        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+    },\n+    {\n+        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n+        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+    }\n+],\n+});\n+\n+async function generateContent() {\n+  const req = {\n+    contents: [{role: 'user', parts: [{text: \"who are you?\"}]}],\n+  };\n+\n+  const streamingResp = await generativeModel.generateContentStream(req);\n+\n+  for await (const item of streamingResp.stream) {\n+    process.stdout.write('stream chunk: ' + JSON.stringify(item));\n+  }\n+\n+  process.stdout.write('aggregated response: ' + JSON.stringify(await streamingResp.response));\n+};\n+\n+\n+\n+// Route for generating content\n+app.get('/generate-content', async (req, res) => {\n+    try {\n+        const reqData = {\n+            contents: [{ role: 'user', parts: [{text: \"who are you?\"}] }],\n+        };\n+\n+        const streamingResp = generateContent();\n+        res.json({ \"done\": 'done' });\n+    } catch (error) {\n+        console.error('Error:', error);\n+        res.status(500).json({ error: 'Internal Server Error' });\n+    }\n+});\n+\n+// Start the server\n+const PORT = process.env.PORT || 3000;\n+app.listen(PORT, () => {\n+    console.log(`Server is running on port ${PORT}`);\n+});\n"
                },
                {
                    "date": 1709750916797,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,8 @@\n const { VertexAI } = require('@google-cloud/vertexai');\n \n const app = express();\n \n-const {VertexAI} = require('@google-cloud/vertexai');\n \n // Initialize Vertex with your Cloud project and location\n const vertex_ai = new VertexAI({project: 'inbound-domain-415706', location: 'us-east4'});\n const model = 'gemini-1.0-pro-vision-001';\n@@ -73,57 +72,4 @@\n const PORT = process.env.PORT || 3000;\n app.listen(PORT, () => {\n     console.log(`Server is running on port ${PORT}`);\n });\n-const express = require('express');\n-const { VertexAI } = require('@google-cloud/vertexai');\n-\n-const app = express();\n-\n-// Initialize VertexAI with your Cloud project and location\n-const vertex_ai = new VertexAI({ project: 'your-project-id', location: 'your-location' });\n-const model = 'your-model-id'; // Specify your model ID\n-\n-// Instantiate the model\n-const generativeModel = vertex_ai.preview.getGenerativeModel({\n-    model: model,\n-    generation_config: {\n-        \"max_output_tokens\": 2048,\n-        \"temperature\": 0.4,\n-        \"top_p\": 1,\n-        \"top_k\": 32\n-    },\n-    safety_settings: [\n-        { \"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n-        { \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n-        { \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n-        { \"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" }\n-    ]\n-});\n-\n-// Route for generating content\n-app.get('/generate-content', async (req, res) => {\n-    try {\n-        const reqData = {\n-            contents: [{ role: 'user', parts: [] }],\n-        };\n-\n-        const streamingResp = await generativeModel.generateContentStream(reqData);\n-        const generatedContent = [];\n-\n-        for await (const item of streamingResp.stream) {\n-            generatedContent.push(item);\n-        }\n-\n-        const aggregatedResponse = await streamingResp.response;\n-        res.json({ generatedContent, aggregatedResponse });\n-    } catch (error) {\n-        console.error('Error:', error);\n-        res.status(500).json({ error: 'Internal Server Error' });\n-    }\n-});\n-\n-// Start the server\n-const PORT = process.env.PORT || 3000;\n-app.listen(PORT, () => {\n-    console.log(`Server is running on port ${PORT}`);\n-});\n"
                },
                {
                    "date": 1709751085117,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,16 +40,19 @@\n async function generateContent() {\n   const req = {\n     contents: [{role: 'user', parts: [{text: \"who are you?\"}]}],\n   };\n+  try {\n+    const streamingResp = await generativeModel.generateContentStream(req);\n \n-  const streamingResp = await generativeModel.generateContentStream(req);\n+    for await (const item of streamingResp.stream) {\n+      process.stdout.write('stream chunk: ' + JSON.stringify(item));\n+    }\n \n-  for await (const item of streamingResp.stream) {\n-    process.stdout.write('stream chunk: ' + JSON.stringify(item));\n+    process.stdout.write('aggregated response: ' + JSON.stringify(await streamingResp.response));\n+  } catch (error) {\n+    console.error('Error generating content:', error);\n   }\n-\n-  process.stdout.write('aggregated response: ' + JSON.stringify(await streamingResp.response));\n };\n \n \n \n"
                },
                {
                    "date": 1709751252063,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,11 +2,10 @@\n const { VertexAI } = require('@google-cloud/vertexai');\n \n const app = express();\n \n-\n // Initialize Vertex with your Cloud project and location\n-const vertex_ai = new VertexAI({project: 'inbound-domain-415706', location: 'us-east4'});\n+const vertex_ai = new VertexAI({ project: 'inbound-domain-415706', location: 'us-east4' });\n const model = 'gemini-1.0-pro-vision-001';\n \n // Instantiate the models\n const generativeModel = vertex_ai.preview.getGenerativeModel({\n@@ -15,64 +14,60 @@\n     \"max_output_tokens\": 2048,\n     \"temperature\": 0.4,\n     \"top_p\": 1,\n     \"top_k\": 32\n-},\n+  },\n   safety_settings: [\n     {\n-        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n-        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n+      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n     },\n     {\n-        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n-        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n+      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n     },\n     {\n-        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n-        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n+      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n     },\n     {\n-        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n-        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n+      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n     }\n-],\n+  ],\n });\n \n async function generateContent() {\n   const req = {\n-    contents: [{role: 'user', parts: [{text: \"who are you?\"}]}],\n+    contents: [{ role: 'user', parts: [{ text: \"who are you?\" }] }],\n   };\n   try {\n     const streamingResp = await generativeModel.generateContentStream(req);\n+    let aggregatedResponse = '';\n \n     for await (const item of streamingResp.stream) {\n-      process.stdout.write('stream chunk: ' + JSON.stringify(item));\n+      aggregatedResponse += JSON.stringify(item);\n     }\n \n-    process.stdout.write('aggregated response: ' + JSON.stringify(await streamingResp.response));\n+    return aggregatedResponse;\n   } catch (error) {\n     console.error('Error generating content:', error);\n+    throw error; // Re-throw the error to handle it in the route handler\n   }\n-};\n+}\n \n-\n-\n // Route for generating content\n app.get('/generate-content', async (req, res) => {\n-    try {\n-        const reqData = {\n-            contents: [{ role: 'user', parts: [{text: \"who are you?\"}] }],\n-        };\n-\n-        const streamingResp = generateContent();\n-        res.json({ \"done\": 'done' });\n-    } catch (error) {\n-        console.error('Error:', error);\n-        res.status(500).json({ error: 'Internal Server Error' });\n-    }\n+  try {\n+    const generatedContent = await generateContent();\n+    res.json({ content: generatedContent });\n+  } catch (error) {\n+    console.error('Error:', error);\n+    res.status(500).json({ error: 'Internal Server Error' });\n+  }\n });\n \n // Start the server\n const PORT = process.env.PORT || 3000;\n app.listen(PORT, () => {\n-    console.log(`Server is running on port ${PORT}`);\n+  console.log(`Server is running on port ${PORT}`);\n });\n"
                },
                {
                    "date": 1709790075459,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,8 +44,9 @@\n     const streamingResp = await generativeModel.generateContentStream(req);\n     let aggregatedResponse = '';\n \n     for await (const item of streamingResp.stream) {\n+      console.log(aggregatedResponse)\n       aggregatedResponse += JSON.stringify(item);\n     }\n \n     return aggregatedResponse;\n"
                },
                {
                    "date": 1709790108501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -44,9 +44,9 @@\n     const streamingResp = await generativeModel.generateContentStream(req);\n     let aggregatedResponse = '';\n \n     for await (const item of streamingResp.stream) {\n-      console.log(aggregatedResponse)\n+      console.log(\"Herte\")\n       aggregatedResponse += JSON.stringify(item);\n     }\n \n     return aggregatedResponse;\n"
                },
                {
                    "date": 1709790144486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,12 +41,13 @@\n     contents: [{ role: 'user', parts: [{ text: \"who are you?\" }] }],\n   };\n   try {\n     const streamingResp = await generativeModel.generateContentStream(req);\n+    console.log(\"Herte\")\n     let aggregatedResponse = '';\n \n     for await (const item of streamingResp.stream) {\n-      console.log(\"Herte\")\n+      \n       aggregatedResponse += JSON.stringify(item);\n     }\n \n     return aggregatedResponse;\n"
                },
                {
                    "date": 1709790367000,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n     contents: [{ role: 'user', parts: [{ text: \"who are you?\" }] }],\n   };\n   try {\n     const streamingResp = await generativeModel.generateContentStream(req);\n-    console.log(\"Herte\")\n+    \n     let aggregatedResponse = '';\n \n     for await (const item of streamingResp.stream) {\n       \n@@ -63,9 +63,9 @@\n     const generatedContent = await generateContent();\n     res.json({ content: generatedContent });\n   } catch (error) {\n     console.error('Error:', error);\n-    res.status(500).json({ error: 'Internal Server Error' });\n+    res.status(500).json({ error });\n   }\n });\n \n // Start the server\n"
                },
                {
                    "date": 1709790561691,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,16 +40,16 @@\n   const req = {\n     contents: [{ role: 'user', parts: [{ text: \"who are you?\" }] }],\n   };\n   try {\n-    const streamingResp = await generativeModel.generateContentStream(req);\n+    const streamingResp = await generativeModel.generateContent(req)\n     \n-    let aggregatedResponse = '';\n+    let aggregatedResponse = streamingResp.response;\n \n-    for await (const item of streamingResp.stream) {\n+    // for await (const item of streamingResp.stream) {\n       \n-      aggregatedResponse += JSON.stringify(item);\n-    }\n+    //   aggregatedResponse += JSON.stringify(item);\n+    // }\n \n     return aggregatedResponse;\n   } catch (error) {\n     console.error('Error generating content:', error);\n"
                },
                {
                    "date": 1709791274486,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,12 +1,13 @@\n const express = require('express');\n const { VertexAI } = require('@google-cloud/vertexai');\n+const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n \n const app = express();\n \n // Initialize Vertex with your Cloud project and location\n const vertex_ai = new VertexAI({ project: 'inbound-domain-415706', location: 'us-east4' });\n-const model = 'gemini-1.0-pro-vision-001';\n+// const model = 'gemini-1.0-pro-vision-001';\n \n // Instantiate the models\n const generativeModel = vertex_ai.preview.getGenerativeModel({\n   model: model,\n@@ -35,8 +36,26 @@\n     }\n   ],\n });\n \n+const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n+\n+// Access your API key as an environment variable (see \"Set up your API key\" above)\n+const genAI = new GoogleGenerativeAI('AIzaSyBDEMB0-yQNWw1JbA5dt74aujGfjSBn9Lo');\n+\n+async function run() {\n+  // For text-only input, use the gemini-pro model\n+  const model = genAI.getGenerativeModel({ model: \"gemini-pro\"});\n+\n+  const prompt = \"Who are you?\"\n+\n+  const result = await model.generateContent(prompt);\n+  const response = await result.response;\n+  const text = response.text();\n+  console.log(text);\n+  return response\n+}\n+\n async function generateContent() {\n   const req = {\n     contents: [{ role: 'user', parts: [{ text: \"who are you?\" }] }],\n   };\n@@ -59,9 +78,9 @@\n \n // Route for generating content\n app.get('/generate-content', async (req, res) => {\n   try {\n-    const generatedContent = await generateContent();\n+    const generatedContent = await run();\n     res.json({ content: generatedContent });\n   } catch (error) {\n     console.error('Error:', error);\n     res.status(500).json({ error });\n"
                },
                {
                    "date": 1709791337609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,45 +1,43 @@\n const express = require('express');\n-const { VertexAI } = require('@google-cloud/vertexai');\n+//const { VertexAI } = require('@google-cloud/vertexai');\n const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n \n const app = express();\n \n // Initialize Vertex with your Cloud project and location\n-const vertex_ai = new VertexAI({ project: 'inbound-domain-415706', location: 'us-east4' });\n+//const vertex_ai = new VertexAI({ project: 'inbound-domain-415706', location: 'us-east4' });\n // const model = 'gemini-1.0-pro-vision-001';\n \n // Instantiate the models\n-const generativeModel = vertex_ai.preview.getGenerativeModel({\n-  model: model,\n-  generation_config: {\n-    \"max_output_tokens\": 2048,\n-    \"temperature\": 0.4,\n-    \"top_p\": 1,\n-    \"top_k\": 32\n-  },\n-  safety_settings: [\n-    {\n-      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n-      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n-    },\n-    {\n-      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n-      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n-    },\n-    {\n-      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n-      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n-    },\n-    {\n-      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n-      \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n-    }\n-  ],\n-});\n+// const generativeModel = vertex_ai.preview.getGenerativeModel({\n+//   model: model,\n+//   generation_config: {\n+//     \"max_output_tokens\": 2048,\n+//     \"temperature\": 0.4,\n+//     \"top_p\": 1,\n+//     \"top_k\": 32\n+//   },\n+//   safety_settings: [\n+//     {\n+//       \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n+//       \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+//     },\n+//     {\n+//       \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n+//       \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+//     },\n+//     {\n+//       \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n+//       \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+//     },\n+//     {\n+//       \"category\": \"HARM_CATEGORY_HARASSMENT\",\n+//       \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n+//     }\n+//   ],\n+// });\n \n-const { GoogleGenerativeAI } = require(\"@google/generative-ai\");\n-\n // Access your API key as an environment variable (see \"Set up your API key\" above)\n const genAI = new GoogleGenerativeAI('AIzaSyBDEMB0-yQNWw1JbA5dt74aujGfjSBn9Lo');\n \n async function run() {\n"
                },
                {
                    "date": 1709791700677,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,12 +45,17 @@\n   const model = genAI.getGenerativeModel({ model: \"gemini-pro\"});\n \n   const prompt = \"Who are you?\"\n \n-  const result = await model.generateContent(prompt);\n+  const result = await model.generateContentStream(prompt);\n+  let text = '';\n+  for await (const chunk of result.stream) {\n+    const chunkText = chunk.text();\n+    console.log(chunkText);\n+    text += chunkText;\n+  }\n   const response = await result.response;\n-  const text = response.text();\n-  console.log(text);\n+  const textResp = response.text();\n   return response\n }\n \n async function generateContent() {\n"
                }
            ],
            "date": 1709643830178,
            "name": "Commit-0",
            "content": "const express = require('express');\nconst { VertexAI } = require('@google-cloud/vertexai');\n\nconst app = express();\n\n// Initialize VertexAI with your Cloud project and location\nconst vertex_ai = new VertexAI({ project: 'your-project-id', location: 'your-location' });\nconst model = 'your-model-id'; // Specify your model ID\n\n// Instantiate the model\nconst generativeModel = vertex_ai.preview.getGenerativeModel({\n    model: model,\n    generation_config: {\n        \"max_output_tokens\": 2048,\n        \"temperature\": 0.4,\n        \"top_p\": 1,\n        \"top_k\": 32\n    },\n    safety_settings: [\n        { \"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n        { \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n        { \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" },\n        { \"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\" }\n    ]\n});\n\n// Route for generating content\napp.get('/generate-content', async (req, res) => {\n    try {\n        const reqData = {\n            contents: [{ role: 'user', parts: [] }],\n        };\n\n        const streamingResp = await generativeModel.generateContentStream(reqData);\n        const generatedContent = [];\n\n        for await (const item of streamingResp.stream) {\n            generatedContent.push(item);\n        }\n\n        const aggregatedResponse = await streamingResp.response;\n        res.json({ generatedContent, aggregatedResponse });\n    } catch (error) {\n        console.error('Error:', error);\n        res.status(500).json({ error: 'Internal Server Error' });\n    }\n});\n\n// Start the server\nconst PORT = process.env.PORT || 3000;\napp.listen(PORT, () => {\n    console.log(`Server is running on port ${PORT}`);\n});\n"
        }
    ]
}